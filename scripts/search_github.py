#!/usr/bin/env python3
"""
GitHub Semantic Search v6
=========================

åœ¨æŒ‡å®š GitHub ä»“åº“ä¸­æœç´¢ä¸ç‰¹å®šä¸»é¢˜ç›¸å…³çš„å†…å®¹ï¼ˆIssues, PRs, Code, Commits, Discussionsï¼‰ã€‚
æ”¯æŒ AI åŒä¹‰è¯æ‰©å±• + å¤šè½®å…³é”®è¯æœç´¢ + ç›¸å…³åº¦è¯„åˆ† + å¹¶è¡Œæœç´¢ã€‚

ç”¨æ³•:
    # Issues æœç´¢ (é»˜è®¤ï¼Œå…¼å®¹ v4/v5)
    python search_github.py --config search_config.json --output results.md

    # Issues + PR æœç´¢ (å¹¶è¡Œ)
    python search_github.py --config search_config.json --search-types issues prs

    # å…¨ç±»å‹å¹¶è¡Œæœç´¢
    python search_github.py --config config.json --search-types issues prs code commits discussions

    # ç¦ç”¨å¹¶è¡Œ (è°ƒè¯•ç”¨)
    python search_github.py --config config.json --search-types issues prs --no-parallel

    # å¢é‡æœç´¢ (ä»ç¼“å­˜æ¢å¤)
    python search_github.py --config config_v2.json --cache-file cache.json --resume

ç¯å¢ƒå˜é‡:
    GITHUB_TOKEN: GitHub Personal Access Token (å…è´¹, æ— éœ€æƒé™)
"""

import argparse
import concurrent.futures
import logging
import os
import sys
import threading
import time as _time

# Add parent directory to path for package imports
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from core.models import SearchConfig
from core.api_client import GitHubApiClient
from core.scorer import KeywordScorer
from core.cache import save_cache, load_cache
from core.report import (
    format_markdown, format_json, format_full_report, format_full_json,
    get_ranked_results,
)
from core.cross_ref import build_cross_references, format_cross_ref_summary
from core.query_builder import build_queries, merge_seed_synonyms
from searchers.issue import IssueSearcher
from searchers.pr import PRSearcher
from searchers.code import CodeSearcher
from searchers.commit import CommitSearcher

log = logging.getLogger("gss")


def _setup_logging(verbose: bool = False, quiet: bool = False):
    """Configure the gss logger hierarchy.

    --verbose: DEBUG level (all details)
    --quiet:   WARNING only (errors and rate-limit waits)
    default:   INFO (progress + results)
    """
    level = logging.DEBUG if verbose else (logging.WARNING if quiet else logging.INFO)
    handler = logging.StreamHandler(sys.stderr)
    handler.setFormatter(logging.Formatter(
        "  %(message)s"  # clean prefix, matches original print style
    ))
    root = logging.getLogger("gss")
    root.setLevel(level)
    root.addHandler(handler)
    # Prevent duplicate logs if called multiple times
    root.propagate = False
from searchers.discussion import DiscussionSearcher


def print_token_hint():
    """Print GITHUB_TOKEN setup instructions."""
    print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚  [æç¤º] æœªæ£€æµ‹åˆ° GITHUB_TOKEN                   â”‚")
    print("â”‚                                                  â”‚")
    print("â”‚  å½“å‰é™åˆ¶: Search 10æ¬¡/åˆ†é’Ÿ, REST 60æ¬¡/å°æ—¶     â”‚")
    print("â”‚  è®¾ç½®å:   Search 30æ¬¡/åˆ†é’Ÿ, REST 5000æ¬¡/å°æ—¶   â”‚")
    print("â”‚                                                  â”‚")
    print("â”‚  å…è´¹ç”Ÿæˆ: https://github.com/settings/tokens    â”‚")
    print("â”‚  æ— éœ€å‹¾é€‰ä»»ä½•æƒé™                                â”‚")
    print("â”‚                                                  â”‚")
    print("â”‚  è®¾ç½®æ–¹æ³•:                                       â”‚")
    print("â”‚    Windows: $env:GITHUB_TOKEN = 'ghp_xxx'        â”‚")
    print("â”‚    Linux:   export GITHUB_TOKEN=ghp_xxx          â”‚")
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print()


def print_dry_run(config: SearchConfig):
    """Display dry-run preview of queries and filters."""
    component = config.component
    qualifiers = config.filter_qualifiers
    component_display = component if component else "(ä¸é™)"
    state_display = config.state_filter or "å…¨éƒ¨"
    print(f"\n{'='*60}")
    print(" DRY-RUN é¢„è§ˆ (ä¸æ‰§è¡Œæœç´¢)")
    print(f"{'='*60}")
    print(f" ä»“åº“: {config.repo}")
    print(f" ç»„ä»¶: {component_display}")
    print(f" ä¸»é¢˜: {config.topic}")
    print(f" çŠ¶æ€: {state_display}")
    if config.date_from or config.date_to:
        d_from = config.date_from or "..."
        d_to = config.date_to or "..."
        print(f" æ—¶é—´: {d_from} ~ {d_to}")
    if config.exclude_issues:
        print(f" æ’é™¤: {config.exclude_issues}")
    kh = len(config.keywords_high)
    km = len(config.keywords_medium)
    kl = len(config.keywords_low)
    print(f"\n å…³é”®è¯ ({kh}H + {km}M + {kl}L):")
    if config.keywords_high:
        print(f"   é«˜: {', '.join(config.keywords_high)}")
    if config.keywords_medium:
        print(f"   ä¸­: {', '.join(config.keywords_medium)}")
    if config.keywords_low:
        print(f"   ä½: {', '.join(config.keywords_low)}")
    nq = len(config.queries)
    print(f"\n å°†å‘é€çš„æŸ¥è¯¢ ({nq} æ¡):")
    seen = set()
    for i, qt in enumerate(config.queries, 1):
        if component:
            q = qt.replace("{component}", component)
        else:
            q = qt.replace("{component}", "").replace("  ", " ").strip()
        if qualifiers:
            q = f"{q} {qualifiers}"
        full = f"repo:{config.repo} is:issue {q}"
        q_norm = " ".join(full.split())
        dup = " (é‡å¤)" if q_norm in seen else ""
        seen.add(q_norm)
        print(f"   [{i}] {full}{dup}")
    print(f"\n{'='*60}")
    print(" ä½¿ç”¨ --dry-run ç¡®è®¤åï¼Œå»æ‰è¯¥å‚æ•°å³å¯æ‰§è¡Œæœç´¢")
    print(f"{'='*60}")


def _write_intermediate_json(searchers: dict, config, args):
    """Output intermediate scored results as JSON for AI review.

    Emits top-N results + borderline items per type, with enough context
    for an AI to judge relevance and adjust scores.
    """
    import json

    intermediate = {
        "version": "v6-intermediate",
        "repo": config.repo,
        "component": config.component,
        "topic": config.topic,
        "instructions": (
            "Review each item. Set 'ai_score' to your assessed relevance "
            "(0-30). Set 'ai_label' to 'relevant', 'noise', or 'borderline'. "
            "Save as JSON and pass back with --score-overrides."
        ),
        "types": {},
    }

    for type_key, searcher in searchers.items():
        results = searcher.results
        if not results:
            continue

        items = sorted(results.values(),
                       key=lambda x: -x.relevance_score)

        # Top 30 + borderline (score 1.0 ~ min_score)
        top = items[:30]
        borderline = [r for r in items[30:]
                      if 1.0 <= r.relevance_score < args.min_score][:20]

        def _item_summary(item):
            """Extract compact summary for AI review."""
            d = {"score": round(item.relevance_score, 1),
                 "matched_keywords": sorted(item.matched_keywords)}
            if hasattr(item, "number"):
                d["number"] = item.number
                d["title"] = item.title
                d["url"] = item.url
            if hasattr(item, "state"):
                d["state"] = getattr(item, "state", "")
            if hasattr(item, "path"):
                d["path"] = item.path
                d["url"] = item.url
            if hasattr(item, "sha") and not hasattr(item, "number"):
                d["sha"] = item.sha[:10]
                d["message"] = item.message[:200] if hasattr(item, "message") else ""
            # Body snippet for context (first 300 chars)
            body = getattr(item, "body", "") or ""
            if body:
                d["body_snippet"] = body[:300]
            return d

        section = {
            "total": len(results),
            "top": [_item_summary(r) for r in top],
        }
        if borderline:
            section["borderline"] = [_item_summary(r) for r in borderline]

        intermediate["types"][type_key] = section

    path = args.intermediate_json
    with open(path, "w", encoding="utf-8") as f:
        json.dump(intermediate, f, ensure_ascii=False, indent=2)
    log.info("[Smart] ä¸­é—´ç»“æœå·²ä¿å­˜åˆ° %s (%d ç§ç±»å‹)",
             path, len(intermediate["types"]))


def _apply_score_overrides(searchers: dict, overrides_path: str):
    """Apply AI-reviewed score overrides to search results.

    Expected JSON format:
    {
      "overrides": {
        "issues": { "123": {"ai_score": 15.0, "ai_label": "relevant"}, ... },
        "prs":    { "456": {"ai_score": 0, "ai_label": "noise"}, ... },
        "code":   { "path/to/file.py": {"ai_score": 8.0}, ... },
        ...
      }
    }
    """
    import json

    if not os.path.exists(overrides_path):
        log.warning("[Smart] åˆ†æ•°ä¿®æ­£æ–‡ä»¶ä¸å­˜åœ¨: %s", overrides_path)
        return

    with open(overrides_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    overrides = data.get("overrides", {})
    total_applied = 0

    for type_key, type_overrides in overrides.items():
        if type_key not in searchers:
            continue
        results = searchers[type_key].results

        for key_str, override in type_overrides.items():
            # Convert key to the right type (int for issues/prs/discussions, str for code/commits)
            if type_key in ("issues", "prs", "discussions"):
                try:
                    key = int(key_str)
                except ValueError:
                    continue
            else:
                key = key_str

            if key not in results:
                continue

            item = results[key]
            if "ai_score" in override:
                old = item.relevance_score
                item.relevance_score = float(override["ai_score"])
                log.debug("[Smart] %s #%s: %.1f â†’ %.1f",
                          type_key, key_str, old, item.relevance_score)
                total_applied += 1

    log.info("[Smart] å·²åº”ç”¨ %d æ¡åˆ†æ•°ä¿®æ­£ (æ¥è‡ª %s)",
             total_applied, overrides_path)


def main():
    parser = argparse.ArgumentParser(
        description="GitHub Semantic Search v6",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    # Basic
    parser.add_argument("--config", "-c", help="JSON é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--repo", default="ROCm/rocm-libraries")
    parser.add_argument("--component", default="")
    parser.add_argument("--topic", default="page fault")
    # Search types
    parser.add_argument("--search-types", nargs="*", default=None,
                        help="æœç´¢ç±»å‹: issues prs code commits discussions")
    # Filters
    parser.add_argument("--state", default="", choices=["open", "closed", ""])
    parser.add_argument("--date-from", default="")
    parser.add_argument("--date-to", default="")
    # Search options
    parser.add_argument("--keywords", nargs="*")
    parser.add_argument("--queries", nargs="*")
    parser.add_argument("--search-comments", action="store_true",
                        help="å¼ºåˆ¶å¯ç”¨ comments æœç´¢ (é»˜è®¤: æœ‰ token æ—¶è‡ªåŠ¨å¯ç”¨)")
    parser.add_argument("--no-comments", action="store_true",
                        help="ç¦ç”¨ comments æœç´¢ (å³ä½¿æœ‰ token)")
    parser.add_argument("--comments-low", type=float, default=3.0)
    parser.add_argument("--comments-high", type=float, default=8.0)
    parser.add_argument("--concurrency", type=int, default=0)
    # Cache
    parser.add_argument("--cache-file", default="")
    parser.add_argument("--resume", action="store_true")
    # Output
    parser.add_argument("--min-score", type=float, default=3.0)
    parser.add_argument("--max-component", type=int, default=10,
                        help="ä»…ç»„ä»¶åŒ¹é…çš„æœ€å¤§æ˜¾ç¤ºæ•° (default: 10)")
    parser.add_argument("--output", "-o", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    # Smart features
    parser.add_argument("--intermediate-json", default="",
                        help="è¾“å‡ºä¸­é—´ç»“æœ JSON (ä¾› AI äºŒæ¬¡å®¡æŸ¥)")
    parser.add_argument("--score-overrides", default="",
                        help="AI åˆ†æ•°ä¿®æ­£ JSON æ–‡ä»¶ (è¦†ç›–æœºå™¨è¯„åˆ†)")
    parser.add_argument("--append-queries", nargs="*", default=None,
                        help="è¿½åŠ æŸ¥è¯¢ (å¤šè½®æœç´¢, ä¸ --resume é…åˆ)")
    parser.add_argument("--json", action="store_true")
    parser.add_argument("--dry-run", action="store_true")
    # Parallel
    parser.add_argument("--no-parallel", action="store_true",
                        help="ç¦ç”¨å¹¶è¡Œæœç´¢ (è°ƒè¯•ç”¨, é»˜è®¤å¤šç±»å‹è‡ªåŠ¨å¹¶è¡Œ)")
    parser.add_argument("--max-pages", type=int, default=3,
                        help="æ¯æ¡æŸ¥è¯¢æœ€å¤§åˆ†é¡µæ•° (é»˜è®¤ 3 = 300 æ¡/æŸ¥è¯¢, "
                             "é˜²æ­¢å®½æ³›æŸ¥è¯¢è€—å°½é…é¢)")
    # Verbosity
    verbosity = parser.add_mutually_exclusive_group()
    verbosity.add_argument("--verbose", "-v", action="store_true",
                           help="è¯¦ç»†è¾“å‡º (DEBUG çº§åˆ«)")
    verbosity.add_argument("--quiet", "-q", action="store_true",
                           help="å®‰é™æ¨¡å¼ (ä»…è­¦å‘Šå’Œé”™è¯¯)")
    args = parser.parse_args()

    # Configure logging before any other work
    _setup_logging(verbose=args.verbose, quiet=args.quiet)

    # Build config
    if args.config:
        config = SearchConfig.from_json(args.config)
    else:
        config = SearchConfig(repo=args.repo, component=args.component, topic=args.topic)
        if args.keywords:
            config.keywords_high = args.keywords
        if args.queries:
            config.queries = args.queries

    # CLI overrides
    if args.state:
        config.state_filter = args.state
    if args.date_from:
        config.date_from = args.date_from
    if args.date_to:
        config.date_to = args.date_to
    if args.search_types:
        config.search_types = args.search_types

    # Max pages per query
    config.max_pages = args.max_pages

    # Append queries (multi-round search)
    if args.append_queries:
        config.queries = list(config.queries) + list(args.append_queries)
        log.info("è¿½åŠ  %d æ¡æŸ¥è¯¢ (æ€»è®¡ %d æ¡)",
                 len(args.append_queries), len(config.queries))

    # Validate
    errors = config.validate()
    if errors:
        for err in errors:
            print(f"[é”™è¯¯] {err}")
        sys.exit(1)

    # ========== Phase 2.5: Seed synonym merge (before keyword cache) ==========
    n_seed = merge_seed_synonyms(config)
    if n_seed:
        print(f"\nğŸŒ± ç§å­è¯åº“è¡¥å……äº† {n_seed} ä¸ªå…³é”®è¯")
        print(f"   å½“å‰: H={len(config.keywords_high)}, "
              f"M={len(config.keywords_medium)}, L={len(config.keywords_low)}")

    # ========== Phase 3: Auto-build queries (if not provided) ==========
    if not config.queries:
        config.queries = build_queries(config)
        if config.queries:
            print(f"\nğŸ”§ è‡ªåŠ¨æ„å»ºäº† {len(config.queries)} æ¡æŸ¥è¯¢ (æ¥è‡ª "
                  f"{len(config.all_keywords)} ä¸ªå…³é”®è¯)")
        else:
            print("[é”™è¯¯] æœªæä¾›æœç´¢æŸ¥è¯¢ï¼Œä¸”æ— æ³•ä»å…³é”®è¯è‡ªåŠ¨ç”Ÿæˆã€‚"
                  "è¯·æä¾›å…³é”®è¯æˆ–æŸ¥è¯¢ã€‚")
            sys.exit(1)

    if not config.all_keywords:
        log.warning("æœªæä¾›è¯„åˆ†å…³é”®è¯ã€‚")
    else:
        total_kw = len(config.all_keywords)
        if total_kw < 10:
            print(f"\nâš ï¸  å…³é”®è¯æ•°é‡åå°‘ ({total_kw} ä¸ª)ï¼Œå»ºè®®è‡³å°‘ 10 ä¸ªã€‚")
            print("   æç¤º: è¯·å‚è€ƒ references/synonyms.md è¿›è¡ŒåŒä¹‰è¯æ‰©å±• (Phase 2)ã€‚")
            print(f"   å½“å‰: H={len(config.keywords_high)}, "
                  f"M={len(config.keywords_medium)}, L={len(config.keywords_low)}")
            print()

    # Dry-run
    if args.dry_run:
        print_dry_run(config)
        sys.exit(0)

    # Token check
    token = os.environ.get("GITHUB_TOKEN", "")
    if not token:
        print_token_hint()
        if args.search_comments:
            log.warning("æ—  token æ—¶ comments æœç´¢å¯èƒ½éå¸¸æ…¢ (REST API 60æ¬¡/å°æ—¶)")

    # Initialize
    api_client = GitHubApiClient(token=token)

    # --- Smart auto-comments ---
    # Default: enable comment search when token exists, unless --no-comments
    if args.no_comments:
        args.search_comments = False
    elif not args.search_comments and token:
        # Auto-detect: check Core API budget before enabling
        core_remaining = api_client.check_core_budget()
        if core_remaining >= 50:
            args.search_comments = True
            log.info("[auto-comments] Core API ä½™é‡å……è¶³ (%d)ï¼Œè‡ªåŠ¨å¯ç”¨ comments æœç´¢",
                     core_remaining)
        else:
            log.warning("[auto-comments] Core API ä½™é‡ä¸è¶³ (%d < 50)ï¼Œè·³è¿‡ comments æœç´¢"
                        "ï¼ˆå¯ç”¨ --search-comments å¼ºåˆ¶å¯ç”¨ï¼‰", core_remaining)
    scorer = KeywordScorer()
    search_types = config.search_types

    # Searcher registry: (type_key, SearcherClass, score_method, supports_comments)
    _SEARCHER_REGISTRY = [
        ("issues",      IssueSearcher,      scorer.score_issues,      True),
        ("prs",         PRSearcher,          scorer.score_prs,         True),
        ("code",        CodeSearcher,        scorer.score_code,        False),
        ("commits",     CommitSearcher,      scorer.score_commits,     False),
        ("discussions", DiscussionSearcher,  scorer.score_discussions, False),
    ]

    # Filter to active types only
    active_registry = [(tk, sc, sf, sup)
                       for tk, sc, sf, sup in _SEARCHER_REGISTRY
                       if tk in search_types]

    # Thread lock for cache file writes (read-modify-write is not atomic)
    _cache_lock = threading.Lock()

    def _run_searcher(type_key, SearcherClass, score_fn, supports_comments):
        """Execute one searcher: cache-load â†’ collect â†’ score â†’ details â†’ cache-save.

        Returns (type_key, searcher, was_resumed, elapsed_seconds).
        """
        t0 = _time.monotonic()
        searcher = SearcherClass(api_client, config.repo)
        was_resumed = False

        # Resume from cache
        if args.resume and args.cache_file:
            loaded = load_cache(args.cache_file, config.repo,
                                searcher.results, type_key=type_key)
            if loaded:
                n = len(searcher.results)
                log.info("[%s] å·²åŠ è½½ %d ä¸ªç¼“å­˜ç»“æœ", type_key, n)
                was_resumed = True

        # Phase 1: collect + score
        searcher.collect(config)
        score_fn(searcher.results, config)

        # Phase 2: fetch details (comments) if requested and supported
        if args.search_comments and supports_comments:
            searcher.fetch_details(
                config,
                low_threshold=args.comments_low,
                high_threshold=args.comments_high,
                concurrency=args.concurrency,
            )
            score_fn(searcher.results, config)

        # Save cache (lock for thread safety: read-modify-write on shared file)
        if args.cache_file:
            with _cache_lock:
                save_cache(searcher.results, config.repo,
                           args.cache_file, type_key=type_key)

        elapsed = _time.monotonic() - t0
        return type_key, searcher, was_resumed, elapsed

    # ========== Execute searchers ==========
    searchers: dict[str, object] = {}
    resumed = False
    use_parallel = len(active_registry) > 1 and not args.no_parallel

    if use_parallel:
        log.info("[å¹¶è¡Œæ¨¡å¼] åŒæ—¶æœç´¢ %d ç§ç±»å‹: %s",
                 len(active_registry),
                 ", ".join(tk for tk, *_ in active_registry))
        t_total = _time.monotonic()

        with concurrent.futures.ThreadPoolExecutor(
                max_workers=len(active_registry)) as pool:
            futures = {
                pool.submit(_run_searcher, tk, sc, sf, sup): tk
                for tk, sc, sf, sup in active_registry
            }
            for future in concurrent.futures.as_completed(futures):
                tk = futures[future]
                try:
                    type_key, searcher, was_resumed, elapsed = future.result()
                    searchers[type_key] = searcher
                    if was_resumed:
                        resumed = True
                    log.info("[%s] å®Œæˆ (%.1fs)", type_key, elapsed)
                except Exception as exc:
                    log.error("[%s] æœç´¢å¤±è´¥: %s", tk, exc)

        wall = _time.monotonic() - t_total
        serial_sum = sum(
            getattr(s, '_elapsed', 0)
            for s in searchers.values()
        )
        log.info("[å¹¶è¡Œæ¨¡å¼] å…¨éƒ¨å®Œæˆ: %.1fs å®é™… / %d ç§ç±»å‹",
                 wall, len(searchers))
    else:
        # Sequential execution (single type or --no-parallel)
        for type_key, SearcherClass, score_fn, supports_comments in active_registry:
            _, searcher, was_resumed, elapsed = _run_searcher(
                type_key, SearcherClass, score_fn, supports_comments)
            searchers[type_key] = searcher
            if was_resumed:
                resumed = True

    # ========== Smart: Intermediate JSON (for AI review) ==========
    if args.intermediate_json:
        _write_intermediate_json(searchers, config, args)

    # ========== Smart: Score overrides (from AI review) ==========
    if args.score_overrides:
        _apply_score_overrides(searchers, args.score_overrides)

    # ========== Cross-references ==========
    # Only run cross-reference analysis when 2+ content types have results
    # Cross-reference: triggered when user requested 2+ search types
    _xref_types_requested = len([t for t in config.search_types
                                 if t in ("issues", "prs", "commits")])
    if _xref_types_requested >= 2:
        xref = build_cross_references(
            issue_results=searchers["issues"].results if "issues" in searchers else None,
            pr_results=searchers["prs"].results if "prs" in searchers else None,
            commit_results=searchers["commits"].results if "commits" in searchers else None,
        )
    else:
        log.info("[äº¤å‰å¼•ç”¨] ç”¨æˆ·ä»…è¯·æ±‚ %d ç§å¯å…³è”ç±»å‹ï¼Œè·³è¿‡ (éœ€è¦ â‰¥2 ç§: issues/prs/commits)",
                 _xref_types_requested)
        xref = {"edges": [], "stats": {"total_edges": 0, "issue_pr_links": 0,
                                        "pr_pr_links": 0, "commit_refs": 0}}

    # ========== Output ==========
    result_kwargs = dict(
        config=config,
        min_score=args.min_score,
        searched_comments=args.search_comments,
        issue_results=searchers["issues"].results if "issues" in searchers else None,
        pr_results=searchers["prs"].results if "prs" in searchers else None,
        code_results=searchers["code"].results if "code" in searchers else None,
        commit_results=searchers["commits"].results if "commits" in searchers else None,
        disc_results=searchers["discussions"].results if "discussions" in searchers else None,
    )

    if args.json:
        output = format_full_json(**result_kwargs)
    else:
        output = format_full_report(**result_kwargs, max_component=args.max_component)
        # Append cross-reference section if there are links
        # Place graph PNG next to the output file
        _img_dir = os.path.dirname(os.path.abspath(args.output)) if args.output else ""
        xref_section = format_cross_ref_summary(
            xref,
            issue_results=searchers["issues"].results if "issues" in searchers else None,
            pr_results=searchers["prs"].results if "prs" in searchers else None,
            commit_results=searchers["commits"].results if "commits" in searchers else None,
            repo=config.repo,
            output_dir=_img_dir,
        )
        if xref_section:
            # Insert cross-reference before footer line ("*Generated by ...")
            _footer_marker = "*Generated by search_github.py"
            _footer_idx = output.rfind(_footer_marker)
            if _footer_idx > 0:
                output = (output[:_footer_idx].rstrip()
                          + "\n\n" + xref_section + "\n\n"
                          + output[_footer_idx:])
            else:
                output = output.rstrip() + "\n\n" + xref_section

    if args.output:
        with open(args.output, "w", encoding="utf-8") as f:
            f.write(output)
        print(f"\nç»“æœå·²ä¿å­˜åˆ°: {args.output}")
    else:
        print("\n" + output)

    # ========== Summary ==========
    _TYPE_LABELS = {
        "issues": "Issues", "prs": "PRs", "code": "Code",
        "commits": "Commits", "discussions": "Discussions",
    }

    print(f"\n{'='*60}")
    print(" æœç´¢å®Œæˆ!")

    if resumed:
        print("   æ¨¡å¼: å¢é‡æœç´¢ (ä»ç¼“å­˜æ¢å¤)")

    exclude = set(config.exclude_issues) if config.exclude_issues else set()

    for type_key, searcher in searchers.items():
        results = searcher.results

        # Apply the same filters used in report generation so console
        # summary matches the Markdown output.
        if type_key in ("issues", "prs"):
            pool = {k: v for k, v in results.items() if k not in exclude}
            if config.state_filter:
                pool = {k: v for k, v in pool.items()
                        if v.state == config.state_filter}
            if config.date_from:
                pool = {k: v for k, v in pool.items()
                        if v.created_at >= config.date_from}
        else:
            pool = results

        n_total = len(pool)
        ranked = [r for r in pool.values() if r.relevance_score >= args.min_score]
        n_ranked = len(ranked)
        n_high = len([r for r in ranked if r.relevance_score >= 8.0])
        label = _TYPE_LABELS.get(type_key, type_key)

        if type_key == "code":
            print(f"   {label}: æœç´¢ {n_total} æ–‡ä»¶, ç›¸å…³ {n_ranked}")
        else:
            print(f"   {label}: æœç´¢ {n_total}, ç›¸å…³ {n_ranked}, é«˜åº¦ç›¸å…³ {n_high}")

    # Issue-specific extras
    if "issues" in searchers and args.search_comments:
        issue_results = searchers["issues"].results
        pool_comments = {k: v for k, v in issue_results.items() if k not in exclude}
        if config.state_filter:
            pool_comments = {k: v for k, v in pool_comments.items()
                             if v.state == config.state_filter}
        n_from_comments = len([i for i in pool_comments.values()
                               if i.matched_in_comments and i.relevance_score >= args.min_score])
        print(f"   é€šè¿‡ comments å‘ç°: {n_from_comments} ä¸ª")

    if args.cache_file and searchers:
        print(f"   ç¼“å­˜å·²ä¿å­˜: {args.cache_file}")

    print(f"{'='*60}")


if __name__ == "__main__":
    main()
